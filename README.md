# ğŸš€ AI Self-Healing Health Service (Kubernetes Autoscaler)

An AI-powered Kubernetes self-healing system that dynamically manages Critical and Non-Critical microservices using real-time metrics and predictive scaling.

This project demonstrates intelligent resource prioritization inside a Kubernetes cluster using:

- AI-based prediction engine
- Live metrics monitoring
- Metrics-server tuning
- Load simulation
- Docker + Minikube deployment
- Real-time scaling observation

------------------------------------------------------------

ğŸ—ï¸ PROJECT STRUCTURE

k8s-ai-autoscaler/
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ hf_deploy/
â”‚       â”œâ”€â”€ deployment.yaml
â”‚       â”œâ”€â”€ service.yaml
â”‚       â”œâ”€â”€ critical-service.yaml
â”‚       â”œâ”€â”€ noncritical-service.yaml
â”‚       â”œâ”€â”€ live_metrics_sender.py
â”‚       â”œâ”€â”€ predictor.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ orchestrator/
â”‚   â””â”€â”€ ai_orchestrator.py
â”‚
â””â”€â”€ README.md

------------------------------------------------------------

ğŸ› ï¸ PREREQUISITES

- Docker Desktop (Running)
- Minikube
- kubectl
- Python 3
- Ubuntu / WSL recommended

------------------------------------------------------------

ğŸš€ STEP-BY-STEP SETUP GUIDE

1ï¸âƒ£ Start Kubernetes Cluster

Open Terminal 1:

minikube start --driver=docker
minikube addons enable metrics-server


2ï¸âƒ£ Build Docker Image (If Rebuilding)

eval $(minikube docker-env)
docker build -t ai-self-healing .


3ï¸âƒ£ Fix Metrics Server (Required for kubectl top)

Enable insecure TLS:

kubectl patch deployment metrics-server -n kube-system \
--type='json' \
-p='[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]'

Set metrics resolution to 15 seconds:

kubectl patch deployment metrics-server -n kube-system \
--type='json' \
-p='[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--metric-resolution=15s"}]'


4ï¸âƒ£ Deploy Services

cd model/hf_deploy

kubectl apply -f deployment.yaml
kubectl apply -f critical-service.yaml
kubectl apply -f noncritical-service.yaml
kubectl apply -f service.yaml

Verify:

kubectl get pods
kubectl top pods


5ï¸âƒ£ Access AI Service

minikube service ai-service

Copy the generated URL.

Example:
http://127.0.0.1:35863

Update this URL inside:

live_metrics_sender.py

Example:
BASE_URL = "http://127.0.0.1:35863"


------------------------------------------------------------

ğŸ§ª LOAD TESTING & MONITORING

Terminal 2 â€” Critical Service

minikube service critical-service


Terminal 3 â€” Non-Critical Service

minikube service noncritical-service


Terminal 4 â€” Live CPU Monitoring

watch -n 1 kubectl top pods


Terminal 5 â€” Start AI Metrics Sender

cd model/hf_deploy
python3 live_metrics_sender.py


------------------------------------------------------------

Manual Load Generation

Replace URL with either Critical or Non-Critical service URL:

while true; do 
curl http://127.0.0.1:36373/predict \
-X POST \
-H "Content-Type: application/json" \
-d '{"features":[80,70,200,1,800,5,80,5,0]}'; 
sleep 0.1; 
done

Modify the URL to test:
- Critical service â†’ to stress critical workload
- Non-Critical service â†’ to stress non-critical workload

------------------------------------------------------------

ğŸ¯ WHAT THIS PROJECT DEMONSTRATES

- Intelligent Kubernetes workload management
- AI-driven service prioritization
- Real-time metrics-based scaling
- Critical vs Non-Critical traffic control
- Self-healing microservice orchestration

------------------------------------------------------------

ğŸ§  CORE COMPONENTS

AI Predictor          â†’ Predicts system stress  
Metrics Sender        â†’ Sends live cluster metrics  
Critical Service      â†’ High priority microservice  
Non-Critical Service  â†’ Lower priority workload  
Metrics Server        â†’ Provides real-time CPU data  
Orchestrator          â†’ Decision-making logic  

------------------------------------------------------------

ğŸ“Œ IMPORTANT NOTES

- Keep terminal open when using `minikube service`
- Docker driver requires tunnel to stay active
- Metrics-server patch required in WSL/Linux
- Ensure Docker Desktop is running before cluster start

------------------------------------------------------------

ğŸ”¥ FUTURE IMPROVEMENTS

- Horizontal Pod Autoscaler integration
- Prometheus + Grafana dashboard
- Reinforcement learning-based scaling
- Multi-node cluster simulation
- Cloud deployment (EKS / GKE)


------------------------------------------------------------

ğŸ—ï¸ ARCHITECTURE DIAGRAM

Below shows how the AI Self-Healing Kubernetes system works.

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        User / Load         â”‚
                    â”‚   (Manual / Auto Traffic)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     Kubernetes Cluster     â”‚
                    â”‚        (Minikube)          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                      â”‚                      â”‚
          â–¼                      â–¼                      â–¼

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Critical App   â”‚   â”‚ Non-Critical App â”‚   â”‚   AI Service     â”‚
 â”‚ (High Priority)â”‚   â”‚ (Low Priority)   â”‚   â”‚ (Prediction API) â”‚
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        â”‚ CPU/Memory usage    â”‚ CPU/Memory usage     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                       â–¼                     â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        Metrics Server          â”‚
                â”‚ (kubectl top / live metrics)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   live_metrics_sender.py   â”‚
                 â”‚ Sends metrics â†’ AI model   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      AI Predictor Model    â”‚
                 â”‚  (Stress / Load Decision) â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      AI Orchestrator       â”‚
                 â”‚ Scale / Control Services   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

------------------------------------------------------------

âš™ï¸ WORKFLOW EXPLANATION

1. User sends traffic to services
2. Critical & Non-Critical pods consume CPU
3. Metrics-server collects live CPU usage
4. live_metrics_sender sends metrics to AI model
5. AI predicts system stress
6. Orchestrator decides scaling/prioritization
7. Kubernetes adjusts workload dynamically

------------------------------------------------------------

ğŸ¯ KEY IDEA

If system load increases:
â†’ Critical service gets priority
â†’ Non-critical service can be throttled
â†’ AI predicts before failure
â†’ System becomes self-healing

------------------------------------------------------------

ğŸ‘¨â€ğŸ’» Author

Monish C 